{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f0f3320",
   "metadata": {},
   "source": [
    "# Real Housewives of Lagos: An Analysis of Twitter's Perception"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aaf9c4a",
   "metadata": {},
   "source": [
    "### 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686e1a19",
   "metadata": {},
   "source": [
    "In this Data analysis Project I mined over 170,000 tweets relating to the Real Housewives of Lagos reality TV show using the Python library Tweepy, then I performed a sentiment analysis on the data using the text blob library in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cddbce03",
   "metadata": {},
   "source": [
    "### Contents\n",
    "1. [Introduction](#1.-Introduction)\n",
    "2. [Data Gathering](#2.-Data-Gathering)\n",
    "3. [Data Assessment and Cleaning](#3.-Data-Assessment-and-Cleaning)\n",
    "4. [Data Preprocessing](4.-Data-Preprocessing)\n",
    "5. [Sentiment Analysis](5.-Sentiment-Analysis)\n",
    "6. [Data Visualization](6.-Data-Visualization)\n",
    "7. [Conclusion](7.-Conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10518250",
   "metadata": {},
   "source": [
    "### 2. Data Gathering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff7e7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import tweepy \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv \n",
    "import re \n",
    "import string \n",
    "import glob  \n",
    "import requests \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import textblob\n",
    "from textblob import TextBlob\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "from emot.emo_unicode import UNICODE_EMOJI\n",
    "\n",
    "import warnings\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa67d60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access keys and codes from Twitter Developer Account\n",
    "consumer_key = 'XXXXXXXXXXXXXXXXXXXXX'\n",
    "consumer_secret = 'XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX'\n",
    "access_key= '##########-XXXXXXXXXXXXXXXXXXXXX'\n",
    "access_secret = 'XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fa0275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass in twitter API authentication key\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_key, access_secret) \n",
    "api = tweepy.API(auth,wait_on_rate_limit=True)\n",
    "sleep_on_rate_limit=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5ec08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timeframe\n",
    "since_start= \"2022-07-01\"\n",
    "since_end = \"2022-07-08\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35df7a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect tweets using the Cursor object and scrape tweets individually:\n",
    "def get_tweets(search_query, num_tweets):\n",
    "    tweet_list = [tweets for tweets in tweepy.Cursor(api.search_tweets,\n",
    "                                    q=search_query,\n",
    "                                    lang=\"en\",\n",
    "                                    since_id = since_start,\n",
    "                                    tweet_mode='extended').items(num_tweets)]\n",
    "    for tweet in tweet_list:\n",
    "        tweet_id = tweet.id # get user_id\n",
    "        created_at = tweet.created_at # get time of tweet\n",
    "        text = tweet.full_text # get the tweet\n",
    "        location = tweet.user.location # get user's location\n",
    "        retweet = tweet.retweet_count # get number of retweets\n",
    "        favorite = tweet.favorite_count # get number of likes\n",
    "        with open('rholagos.csv','a', newline='', encoding='utf-8') as csvFile:\n",
    "            csv_writer = csv.writer(csvFile, delimiter=',') \n",
    "            csv_writer.writerow([tweet_id, created_at, text, location, retweet, favorite]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e65fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create keywords to search for, filter Links, retweets, replies.\n",
    "search_words = \"RHOLagos OR RHOLagosReunion OR Real housewives of lagos OR RHOL OR #realhousewivesoflagos OR #RHOL OR #rholagos OR #rhol\"\n",
    "search_query = search_words + \" -filter:retweets AND -filter:replies\"\n",
    "\n",
    "#  Pass in search query and the number of tweets to retrieve\n",
    "get_tweets(search_query,50000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003ace2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect older tweets using the Cursor object and scrape tweets individually:\n",
    "def get_tweets2(search_query, num_tweets, since_id_num):\n",
    "    tweet_list = [tweets for tweets in tweepy.Cursor(api.search_tweets,\n",
    "                                    q=search_query,\n",
    "                                    lang=\"en\",\n",
    "                                    since_id = since_end,\n",
    "                                    tweet_mode='extended').items(num_tweets)]\n",
    "    for tweet in tweet_list[::-1]:\n",
    "        tweet_id = tweet.id # get user_id\n",
    "        created_at = tweet.created_at # get time of tweet\n",
    "        text = tweet.full_text # get the tweet\n",
    "        location = tweet.user.location # get user's location\n",
    "        retweet = tweet.retweet_count # get number of retweets\n",
    "        favorite = tweet.favorite_count # get number of likes\n",
    "        with open('rholagos2.csv','a', newline='', encoding='utf-8') as csvFile:\n",
    "            csv_writer = csv.writer(csvFile, delimiter=',') \n",
    "            csv_writer.writerow([tweet_id, created_at, text, location, retweet, favorite]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0ca00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create keywords to search for, filter Links, retweets, replies.\n",
    "search_words = \"RHOLagos OR RHOLagosReunion OR Real housewives of lagos OR RHOL OR #realhousewivesoflagos OR #RHOL OR #rholagos OR #rhol\"\n",
    "search_query = search_words + \" -filter:retweets AND -filter:replies\"\n",
    "\n",
    "#  Pass in search query and the number of tweets to retrieve\n",
    "get_tweets2(search_query,50000,since_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5360b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save file to my path and convert to dataframe\n",
    "path = r\"C:\\Users\\THERESA\\Desktop\\Tina Project\"  \n",
    "files = glob.glob(path + \"/*.csv\")\n",
    "\n",
    "tweets = []\n",
    "\n",
    "for file in files:\n",
    "    df = pd.read_csv(file, index_col = None, header = None)\n",
    "    tweets.append(df)\n",
    "    \n",
    "# Merge all dataframes\n",
    "tweets_df = pd.concat(tweets, axis=0, ignore_index = True) \n",
    "\n",
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1132eeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename column names\n",
    "dict = {0: 'Id',1: 'Time_of_tweet',2: 'Tweet', 3: 'Location', 4: 'Retweets', 5: 'Likes'}\n",
    "tweets_df.rename(columns=dict, inplace=True)\n",
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d745e94",
   "metadata": {},
   "source": [
    "### 3. Data Assessment and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b1711b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check first five rows\n",
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2506a415",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.duplicated(subset='Id').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e143afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect DataFrame\n",
    "tweets_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404d2205",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for missing values\n",
    "tweets_df.isna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0dbe5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing locations with \"No location\"\n",
    "tweets_df[\"Location\"]=tweets_df[\"Location\"].fillna('No location')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adf2a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns\n",
    "tweets_df.drop([6,7,8,9,10], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bba60a",
   "metadata": {},
   "source": [
    "### 4. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30ed76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to extract hashtags and remove # with REGEX\n",
    "def getHashtags(tweet):\n",
    "    tweet = tweet.lower()  \n",
    "    tweet = re.findall(r'\\#\\w+',tweet) \n",
    "    return \" \".join(tweet)\n",
    "\n",
    "tweets_df['Hashtags'] = tweets_df['Tweet'].apply(getHashtags)\n",
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0debe07",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags_list = tweets_df['Hashtags'].tolist()\n",
    "\n",
    "# Iterate over all hashtags and split where there is more than one hashtag\n",
    "hashtags = []\n",
    "for item in hashtags_list:\n",
    "    item = item.split()\n",
    "    for i in item:\n",
    "        hashtags.append(i)\n",
    "\n",
    "# Determine Unique count of all hashtags used\n",
    "counts = Counter(hashtags)\n",
    "hashtags_df = pd.DataFrame.from_dict(counts, orient='index').reset_index()\n",
    "hashtags_df.columns = ['Hashtags', 'Count']\n",
    "hashtags_df.sort_values(by='Count', ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5aa879d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for top 10 hashtags\n",
    "hashtags_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f8011f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rhol_cast = [\"Chioma\", \"Laura\", \"Toyin\", \"Iyabo\", \"Maryam\", \"Carolyna\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc67b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to extract rhol casts from each Tweet\n",
    "def getrholcast(tweet):\n",
    "    tweet = tweet.lower() \n",
    "    tweet_tokens = word_tokenize(tweet)\n",
    "    rhol_cast = [char for char in tweet_tokens if char in Rhol_cast] \n",
    "    return \" \".join(rhol_cast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3258c2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract casts to a new column\n",
    "tweets_df['Rhol_cast'] = tweets_df['Tweet'].apply(getrholcast)\n",
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402e593e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to replace characters names with correct spellings\n",
    "def castNames(rhol_cast):\n",
    "    replacements = [('carolyn','carolyna'), ('caroline', 'carolyna'), ('tiannah', 'toyin'), ('mariam', 'maryam')]\n",
    "    for pat,repl in replacements:\n",
    "        rhol_cast = re.sub(pat, repl, rhol_cast)\n",
    "    return rhol_cast\n",
    "tweets_df['Rhol_cast'] = tweets_df['Rhol_cast'].apply(castNames)\n",
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f6f04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cast_list = tweets_df['Rhol_cast'].tolist()\n",
    "\n",
    "# Iterate over all cast names and split where there is more than one cast\n",
    "cast = []\n",
    "for item in cast_list:\n",
    "    item = item.split()\n",
    "    for i in item:\n",
    "        cast.append(i)\n",
    "\n",
    "# Determine Unique count of all cast\n",
    "counts = Counter(cast)\n",
    "cast_df = pd.DataFrame.from_dict(counts, orient='index').reset_index()\n",
    "cast_df.columns = ['Rhol_cast', 'Count']\n",
    "cast_df.sort_values(by='Count', ascending=False, inplace=True)\n",
    "cast_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca8e6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining set containing all stopwords in English \n",
    "stop_words = list(stopwords.words('english'))\n",
    "user_stop_words = [\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \n",
    "                   \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\",\n",
    "                   \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \n",
    "                   \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \n",
    "                   \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \n",
    "                   \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \n",
    "                   \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \n",
    "                   \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\",\n",
    "                   \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\",\n",
    "                   \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \n",
    "                   \"now\",'anyone','today','yesterday','day', 'already','real','housewife', 'housewives', 'lagos']\n",
    "alphabets = list(string.ascii_lowercase)\n",
    "stop_words = stop_words + user_stop_words + alphabets + Rhol_cast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c2a25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "emojis = list(UNICODE_EMOJI.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ace96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preProcess tweet for sentiment analysis\n",
    "def preprocessTweets(tweet):\n",
    "    tweet = tweet.lower()\n",
    "    # Cleaning and removing URLâ€™s\n",
    "    tweet = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', tweet, flags = re.MULTILINE)\n",
    "    # Cleaning and removing repeating characters\n",
    "    tweet = re.sub(r'\\@\\w+|\\#\\w+|\\d+', '', tweet)\n",
    "    # Cleaning and removing the above stop words list from the tweet text\n",
    "    tweet_tokens = word_tokenize(tweet)  \n",
    "    filtered_words = [w for w in tweet_tokens if w not in stop_words]\n",
    "    filtered_words = [w for w in filtered_words if w not in emojis]\n",
    "    # Cleaning and removing punctuations\n",
    "    unpunctuated_words = [w for w in filtered_words if w not in string.punctuation]\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    lemma_words = [lemmatizer.lemmatize(w) for w in unpunctuated_words]\n",
    "    return \" \".join(lemma_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590a4dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a new column called 'Processed Tweets' by applying preprocessed tweets function to the 'Tweet' column.\n",
    "tweets_df['Processed_Tweets'] = tweets_df['Tweet'].apply(preprocessTweets)\n",
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835781f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all tweets into one long string with each word separate with a \"space\"\n",
    "tweets_long_string = tweets_df['Processed_Tweets'].tolist()\n",
    "tweets_long_string = \" \".join(tweets_long_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4120ed5e",
   "metadata": {},
   "source": [
    "### 5. Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d928f9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to obtain Polarity Score\n",
    "def getPolarity(tweet):\n",
    "    return TextBlob(tweet).sentiment.polarity\n",
    "\n",
    "# Define function to obtain Sentiment category\n",
    "def getSentimentTextBlob(polarity):\n",
    "    if polarity < 0:\n",
    "        return \"Negative\"\n",
    "    elif polarity == 0:\n",
    "        return \"Neutral\"\n",
    "    else:\n",
    "        return \"Positive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1412d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the functions to respective columns\n",
    "tweets_df['Polarity']=tweets_df['Processed_Tweets'].apply(getPolarity)\n",
    "tweets_df['Sentiment']=tweets_df['Polarity'].apply(getSentimentTextBlob)\n",
    "tweets_df['Sentiment'].value_counts()\n",
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3b97eb",
   "metadata": {},
   "source": [
    "### 6. Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa59c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the Twitter word cloud object\n",
    "tweet_wc = WordCloud(collocations = False,max_words=400, background_color = 'white').generate(tweets_long_string)\n",
    "\n",
    "# Display the generated Word Cloud\n",
    "plt.imshow(tweet_wc, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517da1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save final file\n",
    "tweets_df.to_csv('Rholreunion_Finall_File.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a609f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_wc.to_file(\"wordcloud.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295942f7",
   "metadata": {},
   "source": [
    "### 7. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb4b046",
   "metadata": {},
   "source": [
    "I exported this file to Power BI where i built a dashboard with it to better display my analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c3060a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
